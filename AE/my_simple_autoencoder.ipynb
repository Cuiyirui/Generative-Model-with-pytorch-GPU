{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image\n",
    "from skimage import io, transform\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define some parameter\n",
    "if not os.path.exists('./results'):\n",
    "    os.mkdir('./results')\n",
    "\n",
    "\n",
    "def to_img(x):\n",
    "    x = x.clamp(0, 1)\n",
    "    x = x.view(x.size(0), 3, 360, 270)\n",
    "    return x\n",
    "num_epochs = 200\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "\n",
    "#transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting ../../MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../MNIST/t10k-labels-idx1-ubyte.gz\n",
      "784\n"
     ]
    }
   ],
   "source": [
    "# load image\n",
    "mnist = input_data.read_data_sets('../../MNIST', one_hot=True)\n",
    "z_dim = 10\n",
    "X_dim = mnist.train.images.shape[1]\n",
    "y_dim = mnist.train.labels.shape[1]\n",
    "h_dim = 128\n",
    "cnt = 0\n",
    "\n",
    "print X_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load auto encoder\n",
    "class autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(X_dim, h_dim),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(True), nn.Linear(64, 12), nn.ReLU(True), nn.Linear(12, 3))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3, 12),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(12, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(True), nn.Linear(h_dim, X_dim), nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "# instantiate model and define optimizer\n",
    "model = autoencoder().cuda()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=learning_rate, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/200], loss:0.0515\n",
      "epoch [2/200], loss:0.0323\n",
      "epoch [3/200], loss:0.0292\n",
      "epoch [4/200], loss:0.0552\n",
      "epoch [5/200], loss:0.0449\n",
      "epoch [6/200], loss:0.0393\n",
      "epoch [7/200], loss:0.0338\n",
      "epoch [8/200], loss:0.0314\n",
      "epoch [9/200], loss:0.0379\n",
      "epoch [10/200], loss:0.0388\n",
      "epoch [11/200], loss:0.0330\n",
      "epoch [12/200], loss:0.0437\n",
      "epoch [13/200], loss:0.0375\n",
      "epoch [14/200], loss:0.0339\n",
      "epoch [15/200], loss:0.0283\n",
      "epoch [16/200], loss:0.0318\n",
      "epoch [17/200], loss:0.0322\n",
      "epoch [18/200], loss:0.0242\n",
      "epoch [19/200], loss:0.0248\n",
      "epoch [20/200], loss:0.0458\n",
      "epoch [21/200], loss:0.0302\n",
      "epoch [22/200], loss:0.0230\n",
      "epoch [23/200], loss:0.0229\n",
      "epoch [24/200], loss:0.0308\n",
      "epoch [25/200], loss:0.0327\n",
      "epoch [26/200], loss:0.0344\n",
      "epoch [27/200], loss:0.0307\n",
      "epoch [28/200], loss:0.0291\n",
      "epoch [29/200], loss:0.0362\n",
      "epoch [30/200], loss:0.0303\n",
      "epoch [31/200], loss:0.0310\n",
      "epoch [32/200], loss:0.0224\n",
      "epoch [33/200], loss:0.0225\n",
      "epoch [34/200], loss:0.0416\n",
      "epoch [35/200], loss:0.0278\n",
      "epoch [36/200], loss:0.0235\n",
      "epoch [37/200], loss:0.0296\n",
      "epoch [38/200], loss:0.0217\n",
      "epoch [39/200], loss:0.0229\n",
      "epoch [40/200], loss:0.0230\n",
      "epoch [41/200], loss:0.0229\n",
      "epoch [42/200], loss:0.0246\n",
      "epoch [43/200], loss:0.0318\n",
      "epoch [44/200], loss:0.0288\n",
      "epoch [45/200], loss:0.0278\n",
      "epoch [46/200], loss:0.0285\n",
      "epoch [47/200], loss:0.0198\n",
      "epoch [48/200], loss:0.0304\n",
      "epoch [49/200], loss:0.0284\n",
      "epoch [50/200], loss:0.0248\n",
      "epoch [51/200], loss:0.0227\n",
      "epoch [52/200], loss:0.0365\n",
      "epoch [53/200], loss:0.0375\n",
      "epoch [54/200], loss:0.0209\n",
      "epoch [55/200], loss:0.0317\n",
      "epoch [56/200], loss:0.0170\n",
      "epoch [57/200], loss:0.0191\n",
      "epoch [58/200], loss:0.0175\n",
      "epoch [59/200], loss:0.0288\n",
      "epoch [60/200], loss:0.0314\n",
      "epoch [61/200], loss:0.0196\n",
      "epoch [62/200], loss:0.0249\n",
      "epoch [63/200], loss:0.0261\n",
      "epoch [64/200], loss:0.0269\n",
      "epoch [65/200], loss:0.0251\n",
      "epoch [66/200], loss:0.0221\n",
      "epoch [67/200], loss:0.0271\n",
      "epoch [68/200], loss:0.0211\n",
      "epoch [69/200], loss:0.0230\n",
      "epoch [70/200], loss:0.0204\n",
      "epoch [71/200], loss:0.0251\n",
      "epoch [72/200], loss:0.0306\n",
      "epoch [73/200], loss:0.0226\n",
      "epoch [74/200], loss:0.0233\n",
      "epoch [75/200], loss:0.0267\n",
      "epoch [76/200], loss:0.0257\n",
      "epoch [77/200], loss:0.0213\n",
      "epoch [78/200], loss:0.0212\n",
      "epoch [79/200], loss:0.0298\n",
      "epoch [80/200], loss:0.0168\n",
      "epoch [81/200], loss:0.0278\n",
      "epoch [82/200], loss:0.0187\n",
      "epoch [83/200], loss:0.0227\n",
      "epoch [84/200], loss:0.0265\n",
      "epoch [85/200], loss:0.0239\n",
      "epoch [86/200], loss:0.0256\n",
      "epoch [87/200], loss:0.0281\n",
      "epoch [88/200], loss:0.0331\n",
      "epoch [89/200], loss:0.0273\n",
      "epoch [90/200], loss:0.0262\n",
      "epoch [91/200], loss:0.0211\n",
      "epoch [92/200], loss:0.0357\n",
      "epoch [93/200], loss:0.0329\n",
      "epoch [94/200], loss:0.0273\n",
      "epoch [95/200], loss:0.0276\n",
      "epoch [96/200], loss:0.0208\n",
      "epoch [97/200], loss:0.0248\n",
      "epoch [98/200], loss:0.0170\n",
      "epoch [99/200], loss:0.0229\n",
      "epoch [100/200], loss:0.0224\n",
      "epoch [101/200], loss:0.0229\n",
      "epoch [102/200], loss:0.0232\n",
      "epoch [103/200], loss:0.0173\n",
      "epoch [104/200], loss:0.0239\n",
      "epoch [105/200], loss:0.0202\n",
      "epoch [106/200], loss:0.0234\n",
      "epoch [107/200], loss:0.0261\n",
      "epoch [108/200], loss:0.0194\n",
      "epoch [109/200], loss:0.0209\n",
      "epoch [110/200], loss:0.0249\n",
      "epoch [111/200], loss:0.0238\n",
      "epoch [112/200], loss:0.0237\n",
      "epoch [113/200], loss:0.0254\n",
      "epoch [114/200], loss:0.0333\n",
      "epoch [115/200], loss:0.0219\n",
      "epoch [116/200], loss:0.0178\n",
      "epoch [117/200], loss:0.0267\n",
      "epoch [118/200], loss:0.0215\n",
      "epoch [119/200], loss:0.0160\n",
      "epoch [120/200], loss:0.0226\n",
      "epoch [121/200], loss:0.0234\n",
      "epoch [122/200], loss:0.0293\n",
      "epoch [123/200], loss:0.0258\n",
      "epoch [124/200], loss:0.0210\n",
      "epoch [125/200], loss:0.0284\n",
      "epoch [126/200], loss:0.0360\n",
      "epoch [127/200], loss:0.0233\n",
      "epoch [128/200], loss:0.0169\n",
      "epoch [129/200], loss:0.0231\n",
      "epoch [130/200], loss:0.0239\n",
      "epoch [131/200], loss:0.0237\n",
      "epoch [132/200], loss:0.0244\n",
      "epoch [133/200], loss:0.0208\n",
      "epoch [134/200], loss:0.0261\n",
      "epoch [135/200], loss:0.0252\n",
      "epoch [136/200], loss:0.0228\n",
      "epoch [137/200], loss:0.0209\n",
      "epoch [138/200], loss:0.0176\n",
      "epoch [139/200], loss:0.0246\n",
      "epoch [140/200], loss:0.0237\n",
      "epoch [141/200], loss:0.0244\n",
      "epoch [142/200], loss:0.0267\n",
      "epoch [143/200], loss:0.0263\n",
      "epoch [144/200], loss:0.0221\n",
      "epoch [145/200], loss:0.0267\n",
      "epoch [146/200], loss:0.0226\n",
      "epoch [147/200], loss:0.0206\n",
      "epoch [148/200], loss:0.0251\n",
      "epoch [149/200], loss:0.0173\n",
      "epoch [150/200], loss:0.0205\n",
      "epoch [151/200], loss:0.0243\n",
      "epoch [152/200], loss:0.0230\n",
      "epoch [153/200], loss:0.0261\n",
      "epoch [154/200], loss:0.0145\n",
      "epoch [155/200], loss:0.0201\n",
      "epoch [156/200], loss:0.0295\n",
      "epoch [157/200], loss:0.0240\n",
      "epoch [158/200], loss:0.0204\n",
      "epoch [159/200], loss:0.0185\n",
      "epoch [160/200], loss:0.0251\n",
      "epoch [161/200], loss:0.0258\n",
      "epoch [162/200], loss:0.0233\n",
      "epoch [163/200], loss:0.0199\n",
      "epoch [164/200], loss:0.0253\n",
      "epoch [165/200], loss:0.0304\n",
      "epoch [166/200], loss:0.0256\n",
      "epoch [167/200], loss:0.0232\n",
      "epoch [168/200], loss:0.0189\n",
      "epoch [169/200], loss:0.0214\n",
      "epoch [170/200], loss:0.0196\n",
      "epoch [171/200], loss:0.0246\n",
      "epoch [172/200], loss:0.0232\n",
      "epoch [173/200], loss:0.0228\n",
      "epoch [174/200], loss:0.0209\n",
      "epoch [175/200], loss:0.0278\n",
      "epoch [176/200], loss:0.0251\n",
      "epoch [177/200], loss:0.0187\n",
      "epoch [178/200], loss:0.0163\n",
      "epoch [179/200], loss:0.0196\n",
      "epoch [180/200], loss:0.0172\n",
      "epoch [181/200], loss:0.0118\n",
      "epoch [182/200], loss:0.0160\n",
      "epoch [183/200], loss:0.0242\n",
      "epoch [184/200], loss:0.0205\n",
      "epoch [185/200], loss:0.0200\n",
      "epoch [186/200], loss:0.0239\n",
      "epoch [187/200], loss:0.0167\n",
      "epoch [188/200], loss:0.0205\n",
      "epoch [189/200], loss:0.0233\n",
      "epoch [190/200], loss:0.0216\n",
      "epoch [191/200], loss:0.0289\n",
      "epoch [192/200], loss:0.0263\n",
      "epoch [193/200], loss:0.0241\n",
      "epoch [194/200], loss:0.0198\n",
      "epoch [195/200], loss:0.0242\n",
      "epoch [196/200], loss:0.0217\n",
      "epoch [197/200], loss:0.0263\n",
      "epoch [198/200], loss:0.0197\n",
      "epoch [199/200], loss:0.0174\n",
      "epoch [200/200], loss:0.0193\n"
     ]
    }
   ],
   "source": [
    "# train network\n",
    "for epoch in range(num_epochs):\n",
    "    for data in dataloader:\n",
    "        img, _ = mnist.train.next_batch(mb_size)\n",
    "        img = img.view(img.size(0), -1)\n",
    "        img = Variable(img).cuda()\n",
    "        # ===================forward=====================\n",
    "        output = model(img)\n",
    "        #print output\n",
    "        loss = criterion(output, img)\n",
    "        # print loss\n",
    "        #input()\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "    print('epoch [{}/{}], loss:{:.4f}'\n",
    "          .format(epoch + 1, num_epochs, loss.data[0]))\n",
    "    if epoch % 10 == 0:\n",
    "        pic = to_img(output.cpu().data)\n",
    "        save_image(pic, './results/image_{}.png'.format(epoch))\n",
    "\n",
    "torch.save(model.state_dict(), './garment_autoencoder.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#show the first image\n",
    "t=data['image']\n",
    "img = img.view(img.size(0), -1)\n",
    "print img.size()\n",
    "\n",
    "print output.size()\n",
    "a=output[1,:]\n",
    "a=a.clamp(0,1)\n",
    "a=a.view(3,270,360)\n",
    "a=a.data.cpu().numpy()*255\n",
    "a=a.astype('uint8') \n",
    "print np.shape(a)\n",
    "a = np.transpose(sample, (0,1,2))\n",
    "print np.shape(a)\n",
    "io.imshow(a)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
