{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.datasets import MNIST\n",
    "import os\n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load image\n",
    "mnist = input_data.read_data_sets('../../MNIST', one_hot=True)\n",
    "z_dim = 10\n",
    "X_dim = mnist.train.images.shape[1]\n",
    "y_dim = mnist.train.labels.shape[1]\n",
    "h_dim = 128\n",
    "cnt = 0\n",
    "num_epochs = 300\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "print X_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('./results/conv/'):\n",
    "    os.mkdir('./results/conv/')\n",
    "\n",
    "\n",
    "def to_img(x):\n",
    "    x = x.clamp(0, 1)\n",
    "    x = x.view(x.size(0), 3, 360, 270)\n",
    "    return x\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define model\n",
    "class autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, stride=3, padding=1),  # b, 16, 10, 10\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=2),  # b, 16, 5, 5\n",
    "            nn.Conv2d(16, 8, 3, stride=3, padding=1),  # b, 8, 3, 3\n",
    "            nn.ReLU(True),\n",
    "            #denn.MaxPool2d(2, stride=1)  # b, 8, 2, 2\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8, 16, 3, stride=3),  # b, 16, 5, 5\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16, 8, 2, stride=2, padding=0),  # b, 8, 15, 15\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(8, 3, 3, stride=3, padding=0),  # b, 1, 28, 28\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = autoencoder().cuda()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
    "                             weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/300], loss:0.6089\n",
      "epoch [2/300], loss:0.0496\n",
      "epoch [3/300], loss:0.0430\n",
      "epoch [4/300], loss:0.0418\n",
      "epoch [5/300], loss:0.0308\n",
      "epoch [6/300], loss:0.0297\n",
      "epoch [7/300], loss:0.0255\n",
      "epoch [8/300], loss:0.0201\n",
      "epoch [9/300], loss:0.0227\n",
      "epoch [10/300], loss:0.0208\n",
      "epoch [11/300], loss:0.0160\n",
      "epoch [12/300], loss:0.0263\n",
      "epoch [13/300], loss:0.0169\n",
      "epoch [14/300], loss:0.0202\n",
      "epoch [15/300], loss:0.0168\n",
      "epoch [16/300], loss:0.0198\n",
      "epoch [17/300], loss:0.0158\n",
      "epoch [18/300], loss:0.0199\n",
      "epoch [19/300], loss:0.0155\n",
      "epoch [20/300], loss:0.0173\n",
      "epoch [21/300], loss:0.0170\n",
      "epoch [22/300], loss:0.0128\n",
      "epoch [23/300], loss:0.0173\n",
      "epoch [24/300], loss:0.0118\n",
      "epoch [25/300], loss:0.0189\n",
      "epoch [26/300], loss:0.0183\n",
      "epoch [27/300], loss:0.0160\n",
      "epoch [28/300], loss:0.0149\n",
      "epoch [29/300], loss:0.0172\n",
      "epoch [30/300], loss:0.0156\n",
      "epoch [31/300], loss:0.0145\n",
      "epoch [32/300], loss:0.0121\n",
      "epoch [33/300], loss:0.0160\n",
      "epoch [34/300], loss:0.0155\n",
      "epoch [35/300], loss:0.0152\n",
      "epoch [36/300], loss:0.0136\n",
      "epoch [37/300], loss:0.0135\n",
      "epoch [38/300], loss:0.0096\n",
      "epoch [39/300], loss:0.0141\n",
      "epoch [40/300], loss:0.0127\n",
      "epoch [41/300], loss:0.0124\n",
      "epoch [42/300], loss:0.0131\n",
      "epoch [43/300], loss:0.0129\n",
      "epoch [44/300], loss:0.0143\n",
      "epoch [45/300], loss:0.0135\n",
      "epoch [46/300], loss:0.0123\n",
      "epoch [47/300], loss:0.0135\n",
      "epoch [48/300], loss:0.0123\n",
      "epoch [49/300], loss:0.0115\n",
      "epoch [50/300], loss:0.0148\n",
      "epoch [51/300], loss:0.0105\n",
      "epoch [52/300], loss:0.0158\n",
      "epoch [53/300], loss:0.0179\n",
      "epoch [54/300], loss:0.0103\n",
      "epoch [55/300], loss:0.0117\n",
      "epoch [56/300], loss:0.0109\n",
      "epoch [57/300], loss:0.0140\n",
      "epoch [58/300], loss:0.0156\n",
      "epoch [59/300], loss:0.0131\n",
      "epoch [60/300], loss:0.0135\n",
      "epoch [61/300], loss:0.0133\n",
      "epoch [62/300], loss:0.0206\n",
      "epoch [63/300], loss:0.0169\n",
      "epoch [64/300], loss:0.0113\n",
      "epoch [65/300], loss:0.0130\n",
      "epoch [66/300], loss:0.0124\n",
      "epoch [67/300], loss:0.0118\n",
      "epoch [68/300], loss:0.0119\n",
      "epoch [69/300], loss:0.0098\n",
      "epoch [70/300], loss:0.0111\n",
      "epoch [71/300], loss:0.0149\n",
      "epoch [72/300], loss:0.0121\n",
      "epoch [73/300], loss:0.0175\n",
      "epoch [74/300], loss:0.0112\n",
      "epoch [75/300], loss:0.0136\n",
      "epoch [76/300], loss:0.0132\n",
      "epoch [77/300], loss:0.0143\n",
      "epoch [78/300], loss:0.0128\n",
      "epoch [79/300], loss:0.0115\n",
      "epoch [80/300], loss:0.0103\n",
      "epoch [81/300], loss:0.0112\n",
      "epoch [82/300], loss:0.0138\n",
      "epoch [83/300], loss:0.0093\n",
      "epoch [84/300], loss:0.0097\n",
      "epoch [85/300], loss:0.0132\n",
      "epoch [86/300], loss:0.0130\n",
      "epoch [87/300], loss:0.0143\n",
      "epoch [88/300], loss:0.0132\n",
      "epoch [89/300], loss:0.0130\n",
      "epoch [90/300], loss:0.0122\n",
      "epoch [91/300], loss:0.0124\n",
      "epoch [92/300], loss:0.0136\n",
      "epoch [93/300], loss:0.0141\n",
      "epoch [94/300], loss:0.0129\n",
      "epoch [95/300], loss:0.0134\n",
      "epoch [96/300], loss:0.0112\n",
      "epoch [97/300], loss:0.0107\n",
      "epoch [98/300], loss:0.0110\n",
      "epoch [99/300], loss:0.0117\n",
      "epoch [100/300], loss:0.0107\n",
      "epoch [101/300], loss:0.0103\n",
      "epoch [102/300], loss:0.0105\n",
      "epoch [103/300], loss:0.0129\n",
      "epoch [104/300], loss:0.0139\n",
      "epoch [105/300], loss:0.0131\n",
      "epoch [106/300], loss:0.0119\n",
      "epoch [107/300], loss:0.0106\n",
      "epoch [108/300], loss:0.0083\n",
      "epoch [109/300], loss:0.0140\n",
      "epoch [110/300], loss:0.0124\n",
      "epoch [111/300], loss:0.0104\n",
      "epoch [112/300], loss:0.0100\n",
      "epoch [113/300], loss:0.0094\n",
      "epoch [114/300], loss:0.0117\n",
      "epoch [115/300], loss:0.0087\n",
      "epoch [116/300], loss:0.0090\n",
      "epoch [117/300], loss:0.0108\n",
      "epoch [118/300], loss:0.0111\n",
      "epoch [119/300], loss:0.0092\n",
      "epoch [120/300], loss:0.0097\n",
      "epoch [121/300], loss:0.0101\n",
      "epoch [122/300], loss:0.0116\n",
      "epoch [123/300], loss:0.0078\n",
      "epoch [124/300], loss:0.0098\n",
      "epoch [125/300], loss:0.0071\n",
      "epoch [126/300], loss:0.0119\n",
      "epoch [127/300], loss:0.0125\n",
      "epoch [128/300], loss:0.0144\n",
      "epoch [129/300], loss:0.0085\n",
      "epoch [130/300], loss:0.0125\n",
      "epoch [131/300], loss:0.0105\n",
      "epoch [132/300], loss:0.0101\n",
      "epoch [133/300], loss:0.0116\n",
      "epoch [134/300], loss:0.0102\n",
      "epoch [135/300], loss:0.0129\n",
      "epoch [136/300], loss:0.0116\n",
      "epoch [137/300], loss:0.0121\n",
      "epoch [138/300], loss:0.0094\n",
      "epoch [139/300], loss:0.0129\n",
      "epoch [140/300], loss:0.0076\n",
      "epoch [141/300], loss:0.0096\n",
      "epoch [142/300], loss:0.0082\n",
      "epoch [143/300], loss:0.0103\n",
      "epoch [144/300], loss:0.0133\n",
      "epoch [145/300], loss:0.0102\n",
      "epoch [146/300], loss:0.0098\n",
      "epoch [147/300], loss:0.0106\n",
      "epoch [148/300], loss:0.0081\n",
      "epoch [149/300], loss:0.0123\n",
      "epoch [150/300], loss:0.0093\n",
      "epoch [151/300], loss:0.0084\n",
      "epoch [152/300], loss:0.0092\n",
      "epoch [153/300], loss:0.0109\n",
      "epoch [154/300], loss:0.0116\n",
      "epoch [155/300], loss:0.0090\n",
      "epoch [156/300], loss:0.0099\n",
      "epoch [157/300], loss:0.0134\n",
      "epoch [158/300], loss:0.0101\n",
      "epoch [159/300], loss:0.0107\n",
      "epoch [160/300], loss:0.0098\n",
      "epoch [161/300], loss:0.0107\n",
      "epoch [162/300], loss:0.0108\n",
      "epoch [163/300], loss:0.0072\n",
      "epoch [164/300], loss:0.0104\n",
      "epoch [165/300], loss:0.0111\n",
      "epoch [166/300], loss:0.0087\n",
      "epoch [167/300], loss:0.0098\n",
      "epoch [168/300], loss:0.0108\n",
      "epoch [169/300], loss:0.0102\n",
      "epoch [170/300], loss:0.0097\n",
      "epoch [171/300], loss:0.0083\n",
      "epoch [172/300], loss:0.0113\n",
      "epoch [173/300], loss:0.0093\n",
      "epoch [174/300], loss:0.0088\n",
      "epoch [175/300], loss:0.0131\n",
      "epoch [176/300], loss:0.0082\n",
      "epoch [177/300], loss:0.0076\n",
      "epoch [178/300], loss:0.0074\n",
      "epoch [179/300], loss:0.0093\n",
      "epoch [180/300], loss:0.0100\n",
      "epoch [181/300], loss:0.0096\n",
      "epoch [182/300], loss:0.0106\n",
      "epoch [183/300], loss:0.0093\n",
      "epoch [184/300], loss:0.0086\n",
      "epoch [185/300], loss:0.0109\n",
      "epoch [186/300], loss:0.0094\n",
      "epoch [187/300], loss:0.0083\n",
      "epoch [188/300], loss:0.0061\n",
      "epoch [189/300], loss:0.0105\n",
      "epoch [190/300], loss:0.0098\n",
      "epoch [191/300], loss:0.0092\n",
      "epoch [192/300], loss:0.0097\n",
      "epoch [193/300], loss:0.0086\n",
      "epoch [194/300], loss:0.0082\n",
      "epoch [195/300], loss:0.0085\n",
      "epoch [196/300], loss:0.0107\n",
      "epoch [197/300], loss:0.0120\n",
      "epoch [198/300], loss:0.0091\n",
      "epoch [199/300], loss:0.0091\n",
      "epoch [200/300], loss:0.0107\n",
      "epoch [201/300], loss:0.0084\n",
      "epoch [202/300], loss:0.0080\n",
      "epoch [203/300], loss:0.0104\n",
      "epoch [204/300], loss:0.0084\n",
      "epoch [205/300], loss:0.0083\n",
      "epoch [206/300], loss:0.0103\n",
      "epoch [207/300], loss:0.0073\n",
      "epoch [208/300], loss:0.0094\n",
      "epoch [209/300], loss:0.0078\n",
      "epoch [210/300], loss:0.0088\n",
      "epoch [211/300], loss:0.0084\n",
      "epoch [212/300], loss:0.0091\n",
      "epoch [213/300], loss:0.0097\n",
      "epoch [214/300], loss:0.0069\n",
      "epoch [215/300], loss:0.0109\n",
      "epoch [216/300], loss:0.0078\n",
      "epoch [217/300], loss:0.0089\n",
      "epoch [218/300], loss:0.0125\n",
      "epoch [219/300], loss:0.0110\n",
      "epoch [220/300], loss:0.0108\n",
      "epoch [221/300], loss:0.0108\n",
      "epoch [222/300], loss:0.0099\n",
      "epoch [223/300], loss:0.0089\n",
      "epoch [224/300], loss:0.0082\n",
      "epoch [225/300], loss:0.0088\n",
      "epoch [226/300], loss:0.0074\n",
      "epoch [227/300], loss:0.0082\n",
      "epoch [228/300], loss:0.0111\n",
      "epoch [229/300], loss:0.0084\n",
      "epoch [230/300], loss:0.0106\n",
      "epoch [231/300], loss:0.0117\n",
      "epoch [232/300], loss:0.0112\n",
      "epoch [233/300], loss:0.0105\n",
      "epoch [234/300], loss:0.0099\n",
      "epoch [235/300], loss:0.0080\n",
      "epoch [236/300], loss:0.0106\n",
      "epoch [237/300], loss:0.0075\n",
      "epoch [238/300], loss:0.0096\n",
      "epoch [239/300], loss:0.0087\n",
      "epoch [240/300], loss:0.0129\n",
      "epoch [241/300], loss:0.0105\n",
      "epoch [242/300], loss:0.0104\n",
      "epoch [243/300], loss:0.0119\n",
      "epoch [244/300], loss:0.0082\n",
      "epoch [245/300], loss:0.0071\n",
      "epoch [246/300], loss:0.0074\n",
      "epoch [247/300], loss:0.0129\n",
      "epoch [248/300], loss:0.0115\n",
      "epoch [249/300], loss:0.0092\n",
      "epoch [250/300], loss:0.0113\n",
      "epoch [251/300], loss:0.0080\n",
      "epoch [252/300], loss:0.0095\n",
      "epoch [253/300], loss:0.0110\n",
      "epoch [254/300], loss:0.0099\n",
      "epoch [255/300], loss:0.0085\n",
      "epoch [256/300], loss:0.0063\n",
      "epoch [257/300], loss:0.0097\n",
      "epoch [258/300], loss:0.0072\n",
      "epoch [259/300], loss:0.0076\n",
      "epoch [260/300], loss:0.0099\n",
      "epoch [261/300], loss:0.0094\n",
      "epoch [262/300], loss:0.0123\n",
      "epoch [263/300], loss:0.0077\n",
      "epoch [264/300], loss:0.0101\n",
      "epoch [265/300], loss:0.0111\n",
      "epoch [266/300], loss:0.0065\n",
      "epoch [267/300], loss:0.0098\n",
      "epoch [268/300], loss:0.0113\n",
      "epoch [269/300], loss:0.0110\n",
      "epoch [270/300], loss:0.0094\n",
      "epoch [271/300], loss:0.0107\n",
      "epoch [272/300], loss:0.0103\n",
      "epoch [273/300], loss:0.0086\n",
      "epoch [274/300], loss:0.0083\n",
      "epoch [275/300], loss:0.0067\n",
      "epoch [276/300], loss:0.0097\n",
      "epoch [277/300], loss:0.0100\n",
      "epoch [278/300], loss:0.0097\n",
      "epoch [279/300], loss:0.0104\n",
      "epoch [280/300], loss:0.0073\n",
      "epoch [281/300], loss:0.0097\n",
      "epoch [282/300], loss:0.0076\n",
      "epoch [283/300], loss:0.0078\n",
      "epoch [284/300], loss:0.0092\n",
      "epoch [285/300], loss:0.0105\n",
      "epoch [286/300], loss:0.0088\n",
      "epoch [287/300], loss:0.0095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [288/300], loss:0.0094\n",
      "epoch [289/300], loss:0.0098\n",
      "epoch [290/300], loss:0.0076\n",
      "epoch [291/300], loss:0.0107\n",
      "epoch [292/300], loss:0.0093\n",
      "epoch [293/300], loss:0.0116\n",
      "epoch [294/300], loss:0.0093\n",
      "epoch [295/300], loss:0.0081\n",
      "epoch [296/300], loss:0.0067\n",
      "epoch [297/300], loss:0.0060\n",
      "epoch [298/300], loss:0.0087\n",
      "epoch [299/300], loss:0.0085\n",
      "epoch [300/300], loss:0.0106\n"
     ]
    }
   ],
   "source": [
    "# train network\n",
    "for it in range(10000):\n",
    "    img, _ = mnist.train.next_batch(batch_size)\n",
    "    img = Variable(torch.from_numpy(img)).cuda()\n",
    "    # ===================forward=====================\n",
    "    output = model(img)\n",
    "    #print output\n",
    "    loss = criterion(output, img)\n",
    "    # print loss\n",
    "    #input()\n",
    "    # ===================backward====================\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # ===================log========================\n",
    "    \n",
    "    if it % batch_size == 0:\n",
    "        epoch = it/batch_size\n",
    "        print('epoch [{}/{}], loss:{:.4f}'\n",
    "          .format(epoch + 1, num_epochs, loss.data[0]))\n",
    "        pic = to_img(output.cpu().data)\n",
    "        save_image(pic, './results/image_{}.png'.format(epoch))\n",
    "\n",
    "torch.save(model.state_dict(), './garment_autoencoder.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
